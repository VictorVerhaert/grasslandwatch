{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5563cc52",
   "metadata": {},
   "source": [
    "# Land Cover Mapping Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e65275",
   "metadata": {},
   "source": [
    "This notebook serves as an example of how to use GFMap and openEO to extract point features for training a machine learning model to do Land Cover Mapping. \n",
    "\n",
    "The example uses the following steps:\n",
    "- Load the labelled points and ditribute them into spatial hexagons.\n",
    "- Define the pre-processing steps for extracting the features from Sentinel-1 and Sentinel-2 data.\n",
    "- Set-up the Sentinel-1 and Sentinel-2 fetchers with GFMap and launch the openEO jobs to fetch the data.\n",
    "- Combine the results from all the batch jobs into one dataframe.\n",
    "- Train a random forrest classifier using the extracted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40d694e",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import openeo\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import geojson\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "from typing import List\n",
    "import logging\n",
    "\n",
    "from openeo_gfmap.manager import _log\n",
    "from openeo_gfmap import TemporalContext, Backend, BackendContext, FetchType\n",
    "from openeo_gfmap.manager.job_splitters import split_job_hex\n",
    "from openeo_gfmap.manager.job_manager import GFMAPJobManager\n",
    "from openeo_gfmap.manager import _log\n",
    "from openeo_gfmap.backend import cdse_connection, vito_connection\n",
    "from openeo_gfmap.fetching import build_sentinel2_l2a_extractor, build_sentinel1_grd_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3d175b",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "_log.setLevel(logging.INFO)\n",
    "\n",
    "stream_handler = logging.StreamHandler()\n",
    "_log.addHandler(stream_handler)\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s|%(name)s|%(levelname)s:  %(message)s')\n",
    "stream_handler.setFormatter(formatter)\n",
    "\n",
    "# Exclude the other loggers from other libraries\n",
    "class MyLoggerFilter(logging.Filter):\n",
    "    def filter(self, record):\n",
    "        return record.name == _log.name\n",
    "\n",
    "stream_handler.addFilter(MyLoggerFilter())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856c5726",
   "metadata": {},
   "source": [
    "## Distribute labelled points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8aa80e",
   "metadata": {},
   "source": [
    "First, we load in a dataset with target labels. In order for the model to work, the target labels need to be integers. Also, we extract some target points from the target polygons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1db8d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_folder = Path(\"resources\")\n",
    "YEAR = 2018\n",
    "\n",
    "input_gpkg = gpd.GeoDataFrame()\n",
    "for file in resource_folder.glob(\"*.gpkg\"):\n",
    "    print(\"Digesting\", file)\n",
    "    input_gpkg = pd.concat([input_gpkg, gpd.read_file(file)], ignore_index=True, sort=False, copy=False)\n",
    "\n",
    "input_gpkg[\"geometry\"] = input_gpkg[\"geometry\"].apply(lambda x: x.centroid)\n",
    "input_gpkg = input_gpkg[['UID',\"CODE_1_18\", \"CODE_2_18\", \"CODE_3_18\", \"CODE_4_18\",'geometry', 'REF_TYPE']]\n",
    "input_gpkg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d9b9d6",
   "metadata": {},
   "source": [
    "To extract the target point features, we use GFMap to distribute the target points over multiple hexagons. Each hexagon extraction will be performed in a separate openeo job. \n",
    "Splitting up jobs is necessary because processing a large area in one job would cause memory issues.\n",
    "\n",
    "We use `split_job_hex` for distributing the target points over multiple hexagons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd6a7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_split = split_job_hex(input_gpkg, max_points=50, grid_resolution=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611a66e0",
   "metadata": {},
   "source": [
    "We then create a dataframe where each row represents a single hexagon, and thus batch_job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec1ed56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_job_dataframe(split_jobs: List[gpd.GeoDataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"Create a dataframe from the split jobs, containg all the necessary information to run the job.\"\"\"\n",
    "    rows = []\n",
    "    for job in split_jobs:\n",
    "        start_date = datetime.datetime(YEAR, 1, 1)\n",
    "        end_date = datetime.datetime(YEAR, 12, 31)\n",
    "        rows.append(pd.Series({\n",
    "            'out_prefix': 'S1S2-stats',\n",
    "            'out_extension': '.csv',\n",
    "            'start_date': start_date,\n",
    "            'end_date': end_date,\n",
    "            'geometry': job.to_json()\n",
    "        }))\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "job_df = create_job_dataframe(input_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb92f234",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_df = job_df.head(1) # testing: only run one job for now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbafc7d2",
   "metadata": {},
   "source": [
    "## Define feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c597df85",
   "metadata": {},
   "source": [
    "Next, we will define wich features we want to extract from openeo.\n",
    "\n",
    "First we define the process graph, except the actual loading of a collection. This will be done by using the GFMap specific methods.\n",
    "\n",
    "The preprocessing is contained in a [seperate .py file](./features.py) so we can use it for inference later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86454dda",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "from features import preprocess_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a246c17",
   "metadata": {},
   "source": [
    "## Fetching the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afe7fab",
   "metadata": {},
   "source": [
    "### Set-up the Sentinel-1 and Sentinel-2 fetchers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b4048e",
   "metadata": {},
   "source": [
    "Next we use the extractor methods of GFMap to load the collection. Using these methods allows the backend independant loading of collections (e.g. wether or not we still have to calculate the backscatter on S1 data or not).\n",
    "\n",
    "The loaded collections are pre-processed and then aggregated for the target points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8885b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentinel2_collection(\n",
    "        row : pd.Series,\n",
    "        connection: openeo.DataCube,\n",
    "        geometry: geojson.FeatureCollection\n",
    "    )-> openeo.DataCube:\n",
    "    bands = [\"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B11\", \"B12\", \"SCL\"]\n",
    "    bands_with_platform = [\"S2-L2A-\" + band for band in bands]\n",
    "\n",
    "    extractor = build_sentinel2_l2a_extractor(\n",
    "        backend_context=BackendContext(Backend(row.backend_name)),\n",
    "        bands=bands_with_platform,\n",
    "        fetch_type=FetchType.POINT,\n",
    "    )\n",
    "\n",
    "    temporal_context = TemporalContext(row.start_date, row.end_date)\n",
    "\n",
    "    s2 = extractor.get_cube(connection, geometry, temporal_context)\n",
    "    # TODO add max_cloud_cover 80\n",
    "    s2 = s2.rename_labels(\"bands\", bands)\n",
    "    return s2\n",
    "\n",
    "def sentinel1_collection(\n",
    "        row: pd.Series,\n",
    "        connection : openeo.DataCube,\n",
    "        geometry: geojson.FeatureCollection,\n",
    "    )-> openeo.DataCube:\n",
    "    bands = [\"VH\", \"VV\"]\n",
    "    bands_with_platform = [\"S1-SIGMA0-\" + band for band in bands]\n",
    "\n",
    "    extractor = build_sentinel1_grd_extractor(\n",
    "        backend_context=BackendContext(Backend(row.backend_name)),\n",
    "        bands=bands_with_platform,\n",
    "        fetch_type=FetchType.POINT,\n",
    "    )\n",
    "\n",
    "    temporal_context = TemporalContext(row.start_date, row.end_date)\n",
    "\n",
    "    s1 = extractor.get_cube(connection, geometry, temporal_context)\n",
    "    s1 = s1.rename_labels(\"bands\", bands)\n",
    "    return s1\n",
    "\n",
    "def load_lc_features(\n",
    "    row: pd.Series,\n",
    "    connection : openeo.DataCube,\n",
    "    **kwargs\n",
    "):\n",
    "    geometry = geojson.loads(row.geometry)\n",
    "    \n",
    "    s2_collection = sentinel2_collection(\n",
    "        row=row,\n",
    "        connection=connection,\n",
    "        geometry=geometry\n",
    "    )\n",
    "\n",
    "    s1_collection = sentinel1_collection(\n",
    "        row=row,\n",
    "        connection=connection,\n",
    "        geometry=geometry\n",
    "    )\n",
    "\n",
    "    features = preprocess_features(s2_collection, s1_collection)\n",
    "\n",
    "    # Currently, aggregate_spatial and vectorcubes do not keep the band names, so we'll need to rename them later on\n",
    "    global final_band_names\n",
    "    final_band_names = [b.name for b in features.metadata.band_dimension.bands]\n",
    "\n",
    "    features = features.aggregate_spatial(geometry, reducer=\"median\")\n",
    "    \n",
    "    job_options = {\n",
    "        \"executor-memory\": \"3G\", # Increase this value if a job fails due to memory issues\n",
    "        \"executor-memoryOverhead\": \"2G\",\n",
    "        \"soft-errors\": True\n",
    "    }\n",
    "\n",
    "    return features.create_job(\n",
    "        out_format=\"csv\",\n",
    "        title=f\"GFMAP_Extraction_{geometry.features[0].properties['h3index']}\",\n",
    "        job_options=job_options,\n",
    "    )\n",
    "\n",
    "# Global variable to store the final band names\n",
    "final_band_names = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6d48fd",
   "metadata": {},
   "source": [
    "### Launch the openEO jobs to fetch the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc6dcc6",
   "metadata": {},
   "source": [
    "In order to launch the jobs, we have to define a function that fill determine the outputfile name and create the job manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716ae4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output_path(\n",
    "    root_folder: Path,\n",
    "    geometry_index: int,\n",
    "    row: pd.Series\n",
    ") -> Path:\n",
    "    features = geojson.loads(row.geometry)\n",
    "    h3index = features[geometry_index].properties['h3index']\n",
    "    result = root_folder / f\"{row.out_prefix}_{h3index}_{geometry_index}{row.out_extension}\"\n",
    "    print(\"output_path:\", result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d289b7",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "base_output_path = Path(\"output\")\n",
    "base_output_path.mkdir(exist_ok=True)\n",
    "\n",
    "timenow = datetime.datetime.now()\n",
    "timestr = timenow.strftime(\"%Y%m%d-%Hh%M\")\n",
    "print(f\"Timestr: {timestr}\")\n",
    "tracking_file = base_output_path / f\"tracking_{timestr}.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f7ab53",
   "metadata": {},
   "outputs": [],
   "source": [
    "manager = GFMAPJobManager(\n",
    "    output_dir=base_output_path / timestr,\n",
    "    output_path_generator=generate_output_path,\n",
    "    poll_sleep=60,\n",
    "    n_threads=1,\n",
    "    collection_id=\"LC_feature_extraction\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628675bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "manager.add_backend(Backend.CDSE, cdse_connection, parallel_jobs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962704d4",
   "metadata": {},
   "source": [
    "We then run the prepared jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f328baa5",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "manager.run_jobs(\n",
    "    job_df,\n",
    "    load_lc_features,\n",
    "    tracking_file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402f621d",
   "metadata": {},
   "source": [
    "## Combine the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc57588",
   "metadata": {},
   "source": [
    "We combine all the different extractions into one dataframe to train and test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d906acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run these lines to post-process older results\n",
    "timestr = \"20240318-20h30\"\n",
    "tracking_file = base_output_path / f\"tracking_{timestr}.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263f5291",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tracker_df = pd.read_csv(tracking_file)\n",
    "df = pd.DataFrame(columns = final_band_names + [\"CODE_1_18\", \"CODE_2_18\", \"CODE_3_18\", \"CODE_4_18\", 'geometry'])\n",
    "\n",
    "for index, row in tracker_df.iterrows():\n",
    "    if row.status == \"finished\":\n",
    "        try:\n",
    "            # Get the target and geometry from the input\n",
    "            geometry = gpd.read_file(row.geometry)\n",
    "            geometry['id'] = geometry['id'].astype(int)\n",
    "            h3index = geometry.iloc[0]['h3index']\n",
    "            filename = f\"S1S2-stats_{h3index}_0.csv\"\n",
    "            target_df = geometry[['id', \"CODE_1_18\", \"CODE_2_18\", \"CODE_3_18\", \"CODE_4_18\", 'geometry']]\n",
    "\n",
    "            # Read the stats\n",
    "            stats_df = pd.read_csv(base_output_path/timestr/filename)\n",
    "            stats_df.columns = ['id'] + final_band_names\n",
    "\n",
    "            # Merge the target and geometry with the stats\n",
    "            stats_df = stats_df.merge(target_df, how='left', on='id')\n",
    "            stats_df = stats_df.drop(columns=['id'])\n",
    "\n",
    "            # Append to the dataframe\n",
    "            df = pd.concat([df, stats_df])\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"File not found: {filename}\")\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7051082",
   "metadata": {},
   "source": [
    "Here we filter out features that contain NaN values. These often correspond to the months January and December."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43034a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop NA columns\n",
    "# nan_columns = df.columns[df.isna().any()].tolist()\n",
    "# print(f\"Dropping columns containing NaN: {nan_columns}\")\n",
    "# df.drop(nan_columns, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3e4cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(base_output_path / timestr / \"features.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce86796",
   "metadata": {},
   "source": [
    "## Training and saving a random forrest model\n",
    "The Following is just an example of local training a random forrest.\n",
    "\n",
    "The model is converted to an ONNX model and saved. ONNX is a format to store machine learning models in a standardized way. This allows us to use the model in other applications, such as the openEO backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e531f795",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from skl2onnx import to_onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf46d2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(base_output_path / timestr / \"features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4fb9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"CODE_1_18\", \"CODE_2_18\", \"CODE_3_18\", \"CODE_4_18\", 'geometry'])\n",
    "X = X.astype(np.float32) # convert to float32 to allow ONNX conversion later on\n",
    "y = df['CODE_1_18'].astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab4fb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, max_features=y.unique().size, random_state=42)\n",
    "rf = rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "print(\"Accuracy on test set: \"+str(accuracy_score(y_test,y_pred))[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb3c54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output_path = base_output_path / \"models\"\n",
    "model_output_path.mkdir(exist_ok=True)\n",
    "\n",
    "onnx = to_onnx(model=rf, name=\"random_forest\", X=X_train.values)\n",
    "\n",
    "with open(base_output_path / \"models\" / \"random_forest.onnx\", \"wb\") as f:\n",
    "    f.write(onnx.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce06488",
   "metadata": {},
   "source": [
    "See the [inference notebook](./lc_inference.ipynb) for an example of how to use the model for inference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openEO Python Client",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
